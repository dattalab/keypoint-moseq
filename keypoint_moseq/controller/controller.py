import os
import tqdm
import polars as pl
import keypoint_moseq as kpms
import numpy as np
import pandas as pd
from textwrap import fill
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Union
from keypoint_moseq.io import _deeplabcut_loader, _sleap_loader, _anipose_loader, _sleap_anipose_loader, _nwb_loader, _facemap_loader, _freipose_loader, _dannce_loader, _name_from_path
from keypoint_moseq.project.kpms_project import KPMSProject
from keypoint_moseq.view.jupyter_display import JupyterDisplay

class Controller:
    def __init__(self, project: KPMSProject, display: JupyterDisplay):
        """Initialize the Controller that manages kpms activities

        Parameters
        ----------
        project : KPMSProject
            The 'model' in MVC that manages CRUD operations for kpms
        display : JupyterDisplay
            The 'view' in MVC that manages information display and user input for kpms
        """
        self.project: KPMSProject = project
        self.display: JupyterDisplay = display

    def set_group_labels(self):
        """Run the group labeling widget to assign experimental group labels to each recording
        in the project.
        """
        recordings: pl.DataFrame = self.project.get_recordings()
        
        if 'group' not in recordings.columns:
            recordings = recordings.with_columns(pl.lit('').alias('group'))

        recordings = recordings.sort(['group', 'name'])
            
        recording_names: np.ndarray[str] = recordings.get_column('name').to_numpy()
        initial_recording_groups: np.ndarray[str] = recordings.get_column('group').to_numpy()
        initial_group_labels = dict(zip(recording_names, initial_recording_groups))

        self.display.start_set_group_labels(initial_group_labels, self._save_group_labels)

    def _save_group_labels(self, group_labels: dict[str, str]):
        """Save the new expermental group labels for each recording in the project.

        Parameters
        ----------
        group_labels: dict[str, str]
            A mapping from recording names to experimental group labels.
        """
        recordings = pl.DataFrame({
            'name': list(group_labels.keys()),
            'group': list(group_labels.values())
        })

        self.project.update_recordings(recordings)

    def load_keypoints(
        self,
        filepath_pattern,
        format,
        extension=None,
        recursive=True,
        path_sep="-",
        path_in_name=False,
        remove_extension=True,
        exclude_individuals=["single"],
    ):
        """
        Load keypoint tracking results from one or more files. Several file
        formats are supported:

        - deeplabcut
            .csv and .h5/.hdf5 files generated by deeplabcut. For single-animal
            tracking, each file yields a single key/value pair in the returned
            `coordinates` and `confidences` dictionaries. For multi-animal tracking,
            a key/vaue pair will be generated for each tracked individual. For
            example the file `two_mice.h5` with individuals "mouseA" and "mouseB"
            will yield the pair of keys `'two_mice_mouseA', 'two_mice_mouseB'`.

        - sleap
            .slp and .h5/.hdf5 files generated by sleap. For single-animal tracking,
            each file yields a single key/value pair in the returned `coordinates`
            and `confidences` dictionaries. For multi-animal tracking, a key/vaue
            pair will be generated for each track. For example a single file called
            `two_mice.h5` will yield the pair of keys `'two_mice_track0',
            'two_mice_track1'`.

        - anipose
            .csv files generated by anipose. Each file should contain five columns
            per keypoint (x,y,z,error,score), plus a last column with the frame
            number. The `score` column is used as the keypoint confidence.

        - sleap-anipose
            .h5/.hdf5 files generated by sleap-anipose. Each file should contain a
            dataset called `'tracks'` with shape (n_frames, 1, n_keypoints, 3). If
            there is also a `'point_scores'` dataset, it will be used as the
            keypoint confidence. Otherwise, the confidence will be set to 1.

        - nwb
            .nwb files (Neurodata Without Borders). Each file should contain
            exactly one `PoseEstimation` object (for multi-animal tracking, each
            animal should be stored in its own .nwb file). The `PoseEstimation`
            object should contain one `PoseEstimationSeries` object for each
            bodypart. Confidence values are optional and will be set to 1 if not
            present.

        - facemap
            .h5 files saved by Facemap. See Facemap documentation for details:
            https://facemap.readthedocs.io/en/latest/outputs.html#keypoints-processing
            The files should have the format::

                [filename].h5
                └──Facemap
                    ├──keypoint1
                    │  ├──x
                    │  ├──y
                    │  └──likelihood
                    ⋮

        - freipose
            .json files saved by FreiPose. Each file should contain a list of dicts
            that each include a "kp_xyz" key with the 3D coordinates for one frame.
            Keypoint scores (saved under "kp_score") are not loaded because they are
            not bounded between 0 and 1, which is required for modeling. Since
            FreiPose does not save the bodypart names, the `bodyparts` return
            value is set to None.

        - dannce
            .mat files saved by Dannce.

        Parameters
        ----------
        filepath_pattern: str or list of str
            Filepath pattern for a set of deeplabcut csv or hdf5 files, or a list
            of such patterns. Filepath patterns can be:

            - single file (e.g. `/path/to/file.csv`)
            - single directory (e.g. `/path/to/dir/`)
            - set of files (e.g. `/path/to/fileprefix*`)
            - set of directories (e.g. `/path/to/dirprefix*`)

        format: str
            Format of the files to load. Must be one of `deeplabcut`, `sleap`,
            `anipose`, or `sleap-anipose`.

        extension: str, default=None
            File extension to use when searching for files. If None, then the
            extension will be inferred from the `format` argument:

            - sleap: 'h5' or 'slp'
            - deeplabcut: 'csv' or 'h5'
            - anipose: 'csv'
            - sleap-anipose: 'h5'
            - nwb: 'nwb'
            - facemap: 'h5'
            - freipose: 'json'
            - dannce: 'mat'

        recursive: bool, default=True
            Whether to search recursively for deeplabcut csv or hdf5 files.

        path_in_name: bool, default=False
            Whether to name the tracking results from each file by the path to the
            file (True) or just the filename (False). If True, the `path_sep`
            argument is used to separate the path components.

        path_sep: str, default='-'
            Separator to use when `path_in_name` is True. For example, if
            `path_sep` is `'-'`, then the tracking results from the file
            `/path/to/file.csv` will be named `path-to-file`. Using `'/'` as the
            separator is discouraged, as it will cause problems saving/loading the
            modeling results to/from hdf5 files.

        remove_extension: bool, default=True
            Whether to remove the file extension when naming the tracking results
            from each file.

        exclude_individuals: list of str, default=["single"]
            List of individuals to exclude from the results. This is only used for
            multi-animal tracking with deeplabcut.

        Returns
        -------
        coordinates: dict
            Dictionary mapping filenames to keypoint coordinates as ndarrays of
            shape (n_frames, n_bodyparts, 2[or 3])

        confidences: dict
            Dictionary mapping filenames to `likelihood` scores as ndarrays of
            shape (n_frames, n_bodyparts)

        bodyparts: list of str
            List of bodypart names. The order of the names matches the order of the
            bodyparts in `coordinates` and `confidences`.
        """
        formats = {
            "deeplabcut": (_deeplabcut_loader, [".csv", ".h5", ".hdf5"]),
            "sleap": (_sleap_loader, [".h5", ".hdf5", ".slp"]),
            "anipose": (_anipose_loader, [".csv"]),
            "sleap-anipose": (_sleap_anipose_loader, [".h5", ".hdf5"]),
            "nwb": (_nwb_loader, [".nwb"]),
            "facemap": (_facemap_loader, [".h5", ".hdf5"]),
            "freipose": (_freipose_loader, [".json"]),
            "dannce": (_dannce_loader, [".mat"]),
        }

        # get format-specific loader and extensions
        assert format in formats, fill(
            f"Unrecognized format '{format}'. Must be one of {list(formats.keys())}"
        )
        loader, extensions = formats[format]

        # optionally override default extension list
        if extension is not None:
            extensions = [extension]

        # optionally add format-specific arguments
        if format == "deeplabcut":
            additional_args = {"exclude_individuals": exclude_individuals}
        else:
            additional_args = {}

        # get list of filepaths
        filepaths = kpms.list_files_with_exts(filepath_pattern, extensions, recursive=recursive)
        assert len(filepaths) > 0, fill(
            f"No files with extensions {extensions} found for {filepath_pattern}"
        )

        # load keypoints from each file
        coordinates, confidences, bodyparts = {}, {}, None
        for filepath in tqdm.tqdm(filepaths, desc=f"Loading keypoints", ncols=72):
            name = _name_from_path(filepath, path_in_name, path_sep, remove_extension)

            try:
                new_coordinates, new_confidences, bodyparts = loader(
                    filepath, name, **additional_args
                )
            except Exception as e:
                print(fill(f"Error loading {filepath}"))
                raise e

            if set(new_coordinates.keys()) & set(coordinates.keys()):
                raise ValueError(
                    f"Duplicate names found in {filepath_pattern}:\n\n"
                    f"{set(new_coordinates.keys()) & set(coordinates.keys())}"
                    f"\n\nThis may be caused by repeated filenames with "
                    "different extensions. If so, please set the extension "
                    "explicitly via the `extension` argument. Another possible"
                    " cause is commonly-named files in different directories. "
                    "if that is the case, then set `path_in_name=True`."
                )

            coordinates.update(new_coordinates)
            confidences.update(new_confidences)

        # check for valid results
        assert len(coordinates) > 0, fill(f"No valid results found for {filepath_pattern}")
        kpms.check_nan_proportions(coordinates, bodyparts)

        recordings = pl.DataFrame({'name': coordinates.keys()})
        self.project.add_recordings(recordings)

        return coordinates, confidences, bodyparts